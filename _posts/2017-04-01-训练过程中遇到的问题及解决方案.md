---
layout: post
title: 训练过程中遇到的问题及解决方案
date: 2017-04-01
#excerpt: "A ton of text to test readability."
tags: 深度学习
comments: true
---

### 在图像分类任务中, 训练数据不足会带来什么问题, 如何缓解数据量不足带来的问题?
```
带来的问题: 过拟合
处理方法

基于模型的方法: 采用降低过拟合风险的措施,包括简化模型(如将非线性简化成线性), 添加约束项以缩小假设空间(如L1和L2正则化), 集成学习, Dropout超参数等.
基于数据的方法, 主要通过数据扩充(Data Augmentation), 即根据一些先验知识, 在保持特定信息的前提下, 对原始数据进行适合变换以达到扩充数据集的效果.
在图像分类任务中，在保持图像类别不变的前提下，可以对训练集中的每幅图像进行以下变换：

观察角度：一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等
噪声扰动：椒盐噪声、高斯白噪声
颜色变换：在RGB颜色空间上进行主成分分析
其他：亮度、清晰度、对比度、锐度
其他扩充数据方法：特征提取, 在图像的特征空间内进行变换：数据扩充or上采样技术，如SMOTE（Synthetic Minority Over-sampling Technique)。

最后，迁移学习或者用GAN合成一些新样本也可帮助解决数据不足问题。
```

### 如何解决数据不均衡问题?
```
解决这一问题的基本思路是让正负样本在训练过程中拥有相同的话语权，比如利用采样与加权等方法。为了方便起见，我们把数据集中样本较多的那一类称为“大众类”，样本较少的那一类称为“小众类”。

1. 采样
采样方法是通过对训练集进行处理使其从不平衡的数据集变成平衡的数据集，在大部分情况下会对最终的结果带来提升。

采样分为上采样（Oversampling）和下采样（Undersampling），上采样是把小种类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。

随机采样最大的优点是简单，但缺点也很明显。上采样后的数据集中会反复出现一些样本，训练出来的模型会有一定的过拟合；而下采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。

上采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动，经验表明这种做法非常有效。

因为下采样会丢失信息，如何减少信息的损失呢？第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。第三种方法是利用KNN试图挑选那些最具代表性的大众样本，叫做NearMiss，这类方法计算量很大，感兴趣的可以参考“Learning from Imbalanced Data”这篇综述的3.2.1节。

2. 数据合成
数据合成方法是利用已有样本生成更多样本，这类方法在小数据场景下有很多成功案例，比如医学图像分析等。

其中最常见的一种方法叫做SMOTE，它利用小众样本在特征空间的相似性来生成新样本。对于小众样本xi∈Smin，从它属于小众类的K近邻中随机选取一个样本点x^i，生成一个新的小众样本xnew：xnew=xi+(x^−xi)×δ，其中δ∈[0,1]是随机数。

一种生成小众样本的方法叫SMOTE

上图是SMOTE方法在K=6近邻下的示意图，黑色方格是生成的新样本。

SMOTE为每个小众样本合成相同数量的新样本，这带来一些潜在的问题：一方面是增加了类之间重叠的可能性，另一方面是生成一些没有提供有益信息的样本。为了解决这个问题，出现两种方法：Borderline-SMOTE与ADASYN。

Borderline-SMOTE的解决思路是寻找那些应该为之合成新样本的小众样本。即为每个小众样本计算K近邻，只为那些K近邻中有一半以上大众样本的小众样本生成新样本。直观地讲，只为那些周围大部分是大众样本的小众样本生成新样本，因为这些样本往往是边界样本。确定了为哪些小众样本生成新样本后再利用SMOTE生成新样本。

ADASYN的解决思路是根据数据分布情况为不同小众样本生成不同数量的新样本。首先根据最终的平衡程度设定总共需要生成的新小众样本数量G，然后为每个小众样本xi计算分布比例Γi：Γi=Δi/KZ，其中Γi是xiK近邻中大众样本的数量，Z用来归一化使得∑Γi=1，最后为小众样本xi生成新样本的个数为gi=Γi×G，确定个数后再利用SMOTE生成新样本。

3. 加权
除了采样和生成新数据等方法，我们还可以通过加权的方式来解决数据不平衡问题，即对不同类别分错的代价不同
```
### 训练不收敛的具体表现是什么? 可能的原因是什么? 如何解决?
```
我们主要通过观察 loss 曲线来判断是否收敛, 根据不同的 loss 曲线, 有以下三种不收敛的情形:

从训练开始曲线就一直震荡或者发散
可能原因: (1) 学习率设置的过大; (2) 向网络中输入的数据是错误数据, 如标签对应错误, 读取图片时将宽高弄反, 图片本身质量极差等;
解决方法: 调节学习率; 检查数据读取代码
在训练过程中曲线突然发散
可能原因: (1) 学习率设置过大, 或者没有采用衰减策略; (2) 读取到了个别的脏数据, 如标签对应错误, 或者标签为空等
解决方法: 调整学习率及相应的衰减策略; 将 batch size 设为 1, shuffle 置为 false, 检查发散时对应的数据是否正确;
在训练过程中曲线突然震荡
可能原因: (1) 损失函数中的正则化系数设置有问题, 或者损失函数本身存在 Bug; (2) 数据存在问题
解决方法: 检查损失函数; 检查数据
```
### 训练过程中出现 Nan 值是什么原因? 如何解决?
```
Nan 是 “Not a number” 的缩写, 出现 Nan 的可能情况一般来说有两种:

一种是梯度爆炸, 使得某一层计算出来的值超过了浮点数的表示范围
另一种是由于损失函数中 log 项的值出现的负值或者 0 导致的, 因为 logx 只在 x 大于 0 的时候才有意义.
解决梯度爆炸的方法通常是对数据进行归一化(BN, GN 等), 减小学习率, 加入 gradient clipping, 更换参数初始化方法(待商榷).

对于 log 项出现负值或者 0 的情况(Softmax 激活函数的取值范围是 [0,1], 因此有可能输出 0), 首先确保网络使用了正确的初始化方法(若参数全为 0, 则输出也为 0), 其次, 检查数据本身是否存在问题, 因为实际业务上的真实数据通常还有大量的脏数据, 有时候数据本身就还有 Nan 值, 因此, 在训练网络之前 先要确保数据是正确的. 可以设计一个简单的小网络, 然后将所有数据跑一遍, 再根据日志信息去除其中的脏数据.

loss 的计算问题, 当我们采用先求取loss的和, 再做归一化除法时, 如果batch_size过大, 那么loss之和可能会超过数据类型的表示上限, 此时, 有两种解决方法, 一种是将数据类型的表示范围提高, 例如将float变成double, 但是这样也不保险, 较好的做法是在计算loss时, 避免添加操作, 或者预先估计loss的大小
```
### 过拟合是什么? 如何处理过拟合?
```
过拟合定义: 当模型在训练数据上拟合的非常好, 但是在训练数据意外的测试集上却不能很好的拟合数据, 此时就说明这个模型出现的过拟合现象.

解决方法:

使用正则项(Regularization): L1, L2 正则
数据增广(Data Augmentation): 水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转等等, 也可利用GAN辅助生成(不常用)
Dropout: Dropout是指在深度网络的训练中, 以一定的概率随机的”临时丢弃”一部分神经元节点. 具体来讲, Dropout作用于每份小批量训练数据, 由于其随机丢弃部分神经元的机制, 相当于每次迭代都在训练不同结构的神经网络, 可以被认为是一种实用的大规模深度神经网络的模型继承算法. 对于包含 N 个神经元节点的网络, 在Dropout的作用下可以看作为 2N 个模型的集成, 这 2N 个模型可认为是原始网络的子网络, 它们共享部分权值, 并且拥有相同的网络层数, 而模型整个的参数数目不变, 大大简化了运算. 对于任意神经元来说, 每次训练中都与一组随机挑选的不同的神经元集合共同进行优化, 这个过程会减弱全体神经元之间的联合适应性, 减少过拟合的风险, 增强泛化能力. 工作原理和实现: 应用Dropout包括训练和预测两个阶段, 在训练阶段中, 每个神经元节点需要增加一个概率系数, 在前向传播时, 会以这个概率选择是否丢弃当前的神经元. 在测试阶段的前向传播计算时, 每个神经元的参数都会预先乘以概率系数p, 以恢复在训练中该神经元只有p的概率被用于整个神经网络的前向传播计算
Drop Connect: Drop Connect 是另一种减少算法过拟合的正则化策略，是 Dropout 的一般化。在 Drop Connect 的过程中需要将网络架构权重的一个随机选择子集设置为零，取代了在 Dropout 中对每个层随机选择激活函数的子集设置为零的做法。由于每个单元接收来自过去层单元的随机子集的输入，Drop Connect 和 Dropout 都可以获得有限的泛化性能 [22]。Drop Connect 和 Dropout 相似的地方在于它涉及在模型中引入稀疏性，不同之处在于它引入的是权重的稀疏性而不是层的输出向量的稀疏性。
早停: 早停法可以限制模型最小化代价函数所需的训练迭代次数。早停法通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。早停法通过确定迭代次数解决这个问题，不需要对特定值进行手动设置。
```
### 欠拟合是什么? 如何处理欠拟合?
```
过拟合定义: 当模型在训练数据和测试数据上都无法很好的拟合数据时, 说明出现了欠拟合

解决方法:

首先看看是否是神经网络本身的拟合能力不足导致的, 具体方法是让神经网络在每次训练时, 只迭代 同样的数据, 甚至每一个 batch 里面也是完全相同一模一样的数据, 再来看看 loss 值和 accurancy 值的变化. 如果这时候 loss 开始下降, accurancy 也开始上升了, 并且在训练了一段时间后神经网络能够正确地计算出所训练样本的输出值, 那么这种情况属于神经网络拟合能力不足. 因为对于大量的数据样本, 神经网络由于自身能力的原因无法去拟合全部数据, 只能拟合大量样本的整体特征, 或者少数样本的具体特征. 对于拟合能力不足问题, 通常可以增加网络层数, 增加神经元个数, 增大卷积核通道数等方法.
如果不是拟合能力不足导致的欠拟合, 就需要尝试其他方法, 更改网络初始化方法(Xavier, MSRA), 更改优化器, 降低学习率
```
### Dropout 的实现方式在训练阶段和测试阶段有什么不同? 如何保持训练和测试阶段的一致性?
```
Dropout 的实现方式有两种:

直接 Dropout: 使用较少, AlexNet 使用的是这种Dropout. 该方法在训练阶段会按照保留概率来决定是否将神经元的激活值置为0. 同时, 为了保持训练阶段和测试阶段数值的一致性, 会在测试阶段对所有的计算结果乘以保留概率.
Inverted Dropout: 这是目前常用的方法. 该方法在训练阶段会按照保留概率来决定是否将神经元的激活值置为0, 并且, 在训练阶段会令输出值都会乘以 1αdropout, 这样一来, 在训练阶段可以随时更改 dropout 的参数值, 而对于测试阶段来说, 无需对神经元进行任何额外处理, 所有的神经元都相当于适配了训练过程中 dropout 对参数数值大小带来的影响.
```
